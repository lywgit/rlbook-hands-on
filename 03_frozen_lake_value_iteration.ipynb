{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOd291EHn//+rSDjV2SFtlt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lywgit/rlbook-hands-on/blob/main/03_frozen_lake_value_iteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 03 value iteration for Frozenlake game\n",
        "\n",
        "- Bellman optimality equation\n",
        "$$V_s = \\max_{a\\in A} \\mathbb{E}_{s^\\prime\\sim S}[r_{s,a}+\\gamma V_{s^\\prime}] = \\max_{a\\in A} \\sum_{s^\\prime \\in S}p_{a,s→s^\\prime}(r_{s,a} + \\gamma V_{s^\\prime}) $$\n",
        "- value iteration\n",
        "    - state value $V$\n",
        "    - action value $Q$\n"
      ],
      "metadata": {
        "id": "lfufZNuJSHrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium\n",
        "!pip install ale-py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPMqf4X1toQ0",
        "outputId": "2cafde7c-96c0-4f84-b0cf-5e7e349b0931"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n",
            "Collecting ale-py\n",
            "  Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: numpy>1.20 in /usr/local/lib/python3.10/dist-packages (from ale-py) (1.26.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py) (4.12.2)\n",
            "Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ale-py\n",
            "Successfully installed ale-py-0.10.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import gymnasium as gym\n",
        "# gym.pprint_registry()"
      ],
      "metadata": {
        "id": "zf2_MT9etrFo"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Frozenlake game visual\n",
        "- 這個遊戲 action 的結果有機率跟你選的不一樣（會滑）"
      ],
      "metadata": {
        "id": "PKRs-Uo01PGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp_env = gym.make('FrozenLake-v1', render_mode='rgb_array')\n",
        "temp_env.reset()\n",
        "temp_env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "ij1AnPoO1Mck",
        "outputId": "7789eaf6-c5bc-4b77-b5a9-e0dcfc66037b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[180, 200, 230],\n",
              "        [180, 200, 230],\n",
              "        [180, 200, 230],\n",
              "        ...,\n",
              "        [180, 200, 230],\n",
              "        [180, 200, 230],\n",
              "        [180, 200, 230]],\n",
              "\n",
              "       [[180, 200, 230],\n",
              "        [204, 230, 255],\n",
              "        [204, 230, 255],\n",
              "        ...,\n",
              "        [204, 230, 255],\n",
              "        [204, 230, 255],\n",
              "        [180, 200, 230]],\n",
              "\n",
              "       [[180, 200, 230],\n",
              "        [235, 245, 249],\n",
              "        [204, 230, 255],\n",
              "        ...,\n",
              "        [204, 230, 255],\n",
              "        [204, 230, 255],\n",
              "        [180, 200, 230]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[180, 200, 230],\n",
              "        [235, 245, 249],\n",
              "        [235, 245, 249],\n",
              "        ...,\n",
              "        [204, 230, 255],\n",
              "        [235, 245, 249],\n",
              "        [180, 200, 230]],\n",
              "\n",
              "       [[180, 200, 230],\n",
              "        [235, 245, 249],\n",
              "        [235, 245, 249],\n",
              "        ...,\n",
              "        [204, 230, 255],\n",
              "        [204, 230, 255],\n",
              "        [180, 200, 230]],\n",
              "\n",
              "       [[180, 200, 230],\n",
              "        [180, 200, 230],\n",
              "        [180, 200, 230],\n",
              "        ...,\n",
              "        [180, 200, 230],\n",
              "        [180, 200, 230],\n",
              "        [180, 200, 230]]], dtype=uint8)"
            ],
            "text/html": [
              "<style>\n",
              "      .ndarray_repr .ndarray_raw_data {\n",
              "        display: none;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_raw_data {\n",
              "        display: block;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_image_preview {\n",
              "        display: none;\n",
              "      }\n",
              "      </style>\n",
              "      <div id=\"id-6900fc0a-f61d-43cd-ab1c-563816dc3a43\" class=\"ndarray_repr\"><pre>ndarray (256, 256, 3) <button style=\"padding: 0 2px;\">show data</button></pre><img src=\"data:image/png;base64,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\" class=\"ndarray_image_preview\" /><pre class=\"ndarray_raw_data\">array([[[180, 200, 230],\n",
              "        [180, 200, 230],\n",
              "        [180, 200, 230],\n",
              "        ...,\n",
              "        [180, 200, 230],\n",
              "        [180, 200, 230],\n",
              "        [180, 200, 230]],\n",
              "\n",
              "       [[180, 200, 230],\n",
              "        [204, 230, 255],\n",
              "        [204, 230, 255],\n",
              "        ...,\n",
              "        [204, 230, 255],\n",
              "        [204, 230, 255],\n",
              "        [180, 200, 230]],\n",
              "\n",
              "       [[180, 200, 230],\n",
              "        [235, 245, 249],\n",
              "        [204, 230, 255],\n",
              "        ...,\n",
              "        [204, 230, 255],\n",
              "        [204, 230, 255],\n",
              "        [180, 200, 230]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[180, 200, 230],\n",
              "        [235, 245, 249],\n",
              "        [235, 245, 249],\n",
              "        ...,\n",
              "        [204, 230, 255],\n",
              "        [235, 245, 249],\n",
              "        [180, 200, 230]],\n",
              "\n",
              "       [[180, 200, 230],\n",
              "        [235, 245, 249],\n",
              "        [235, 245, 249],\n",
              "        ...,\n",
              "        [204, 230, 255],\n",
              "        [204, 230, 255],\n",
              "        [180, 200, 230]],\n",
              "\n",
              "       [[180, 200, 230],\n",
              "        [180, 200, 230],\n",
              "        [180, 200, 230],\n",
              "        ...,\n",
              "        [180, 200, 230],\n",
              "        [180, 200, 230],\n",
              "        [180, 200, 230]]], dtype=uint8)</pre></div><script>\n",
              "      (() => {\n",
              "      const titles = ['show data', 'hide data'];\n",
              "      let index = 0\n",
              "      document.querySelector('#id-6900fc0a-f61d-43cd-ab1c-563816dc3a43 button').onclick = (e) => {\n",
              "        document.querySelector('#id-6900fc0a-f61d-43cd-ab1c-563816dc3a43').classList.toggle('show_array');\n",
              "        index = (++index) % 2;\n",
              "        document.querySelector('#id-6900fc0a-f61d-43cd-ab1c-563816dc3a43 button').textContent = titles[index];\n",
              "        e.preventDefault();\n",
              "        e.stopPropagation();\n",
              "      }\n",
              "      })();\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Value iteration algorithm\n",
        "\n",
        "- 一個 episode = 玩完一次遊戲 (成功、失敗、時間到)\n",
        "- 一個 step = 自 state $s$ 出發採取 action $a$ 得到環境反饋的獎勵 reward $r_{s,a}$, 並進到下一個 state $s^\\prime$\n",
        "    - 轉移機率：因為環境不一定是 deterministic, 在相同的 state $s$ 下採取同一種 action 不一定會有一樣的結果，以機率分佈 $p_{a,s→s^\\prime}$ 表示\n",
        "- value iteration algorithm 允許利用已知的「轉移機率」和「獎勵值」資料來計算馬可夫決策過程中的「狀態值 $V$」和「行動值 $Q$」\n",
        "- 如果一個遊戲回合很長或是不會結束的話，每個 step 累積出來的總報酬可能會非常大，因此會用折扣因子 discount factor $\\gamma$ 來降低未來 time step 的貢獻\n",
        "    - $\\gamma$ 介於 0 到 1 之間, 0 代表完全只考慮當下, 1 代表未來的 reward 不打折\n",
        "\n",
        "### Learning state value $V$\n",
        "\n",
        "Iteration: for each state $s$, update\n",
        "$$V_s ← \\max_{a} \\sum_{s^\\prime} p_{a,s→s^\\prime} (r_{s,a} +\\gamma V_{s^\\prime})$$\n",
        "\n",
        "- state $s$ 對應的 state value $V_s$ 代表處於 state $s$ 時預期可以得到的最佳報酬，包含兩部分:\n",
        "    1. 馬上可能取得的獎勵 $r_{s,a}$\n",
        "        - 處於 $s$ 時, 可以考慮所有可採取的行動及將拿到的報酬 $r_{s,a}$, 其中最高的 $r_{s,a}$ 就代表 $s$ 的價值\n",
        "        - 當你進到 $s$, 只要採取對的行動就可以得到這個最高報酬, 代表 state $s$ 有這個價值\n",
        "        - 實作中透過不斷跟環境度互動取得各種 $(s, a) → r_{s,a}$ 資料\n",
        "    2. 下一個狀態的長期（折扣）獎勵 $s^{\\prime}$ 的價值 $\\gamma V_{s^\\prime}$\n",
        "        - 除了拿到 $r_{s,a}$ 之外, 進到 $s^\\prime$ 後將能取得 $V_{s^\\prime}$, 也要考慮進去\n",
        "\n",
        "\n",
        "### Learning Value of Action $Q$\n",
        "\n",
        "Iteration: for each state $s$ and action $a$, update\n",
        "$$Q_{s,a} ← \\sum_{s^\\prime} p_{a,s→s^\\prime} (r_{s,a} + \\gamma \\max_{a^\\prime} Q_{s^\\prime,a^{\\prime}})$$\n",
        "\n",
        "- 估計 state value $V$ 後續也是用於決定在 $s$ 時應該採取的最佳 $a$, 不如更直接的去評估行動的價值 value of action $Q_{s,a}$\n",
        "- $Q_{s,a}$ 代表在 $s$ 的情況下採取行動 $a$ 的預期報酬，包含兩部分：\n",
        "    1. 馬上可能取得的獎勵 $r_{s,a}$\n",
        "    2. 下一個狀態的長期（折扣）獎勵 $\\gamma \\max_{a^\\prime} Q_{s^\\prime,a^{\\prime}}$\n",
        "        - 就是 state value $V_{s^\\prime}$, 但用 $\\max_{a^{\\prime}} Q_{s^\\prime,a^{\\prime}}$  表達及計算\n"
      ],
      "metadata": {
        "id": "uHoYEDm5m9oO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## V iteration code"
      ],
      "metadata": {
        "id": "uylZqkR65GSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# copied and modified from the rlbook Chap5: frozenlake_v_iteration.py\n",
        "\n",
        "import gymnasium as gym\n",
        "import collections\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "ENV_NAME = \"FrozenLake-v1\"\n",
        "GAMMA = 0.9\n",
        "TEST_EPISODES = 20\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.env = gym.make(ENV_NAME)\n",
        "        self.state, _ = self.env.reset()\n",
        "        self.rewards = collections.defaultdict(float) # 紀錄獎勵資料，用於更新 V\n",
        "        self.transits = collections.defaultdict(collections.Counter) # 用於估計 transition 分佈，在 sum over s' 時做加權\n",
        "        self.values = collections.defaultdict(float) # 這裡的 value 是 state value\n",
        "\n",
        "    def play_n_random_steps(self, count): # 用來收集訓練資料\n",
        "        for _ in range(count):\n",
        "            action = self.env.action_space.sample() # 以 random action 暴力累積資料，沒有策略\n",
        "            new_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "            is_done = terminated or truncated\n",
        "            self.rewards[(self.state, action, new_state)] = reward # dict: (s, a, s') -> r\n",
        "            self.transits[(self.state, action)][new_state] += 1    # dict: (s, a) -> dict: s' -> count 紀錄的發生次數\n",
        "            if is_done:\n",
        "                self.state, _ = self.env.reset()\n",
        "            else:\n",
        "                self.state = new_state\n",
        "\n",
        "    def calc_action_value(self, state, action): # 從目前累積的 transit & reward 資料計算 Q(s,a)\n",
        "        target_counts = self.transits[(state, action)] # target_counts 是 dict: s'-> count\n",
        "        total = sum(target_counts.values())\n",
        "        action_value = 0.0\n",
        "        for tgt_state, count in target_counts.items(): # 考慮資料中有發生過的 transition\n",
        "            reward = self.rewards[(state, action, tgt_state)] # 取得 reward\n",
        "            action_value += (count / total) * (reward + GAMMA * self.values[tgt_state]) # 以發生機率 p_s,a,s' ~ (count_s,a,s' / sum(count_s,a)) 加權平均 reward\n",
        "        return action_value\n",
        "\n",
        "    def select_action(self, state):\n",
        "        # state 下的 best action = action 中得到 reward 最高的\n",
        "        best_action, best_value = None, None\n",
        "        for action in range(self.env.action_space.n):\n",
        "            action_value = self.calc_action_value(state, action)\n",
        "            if best_value is None or best_value < action_value:\n",
        "                best_value = action_value\n",
        "                best_action = action\n",
        "        return best_action\n",
        "\n",
        "    def play_episode(self, env):\n",
        "        total_reward = 0.0\n",
        "        state, _ = env.reset()\n",
        "        while True:\n",
        "            action = self.select_action(state)\n",
        "            new_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            is_done = terminated or truncated\n",
        "            self.rewards[(state, action, new_state)] = reward\n",
        "            self.transits[(state, action)][new_state] += 1\n",
        "            total_reward += reward\n",
        "            if is_done:\n",
        "                break\n",
        "            state = new_state\n",
        "        return total_reward\n",
        "\n",
        "    def value_iteration(self):\n",
        "        for state in range(self.env.observation_space.n):\n",
        "            # iteratively update values\n",
        "            state_values = [self.calc_action_value(state, action)\n",
        "                            for action in range(self.env.action_space.n)]\n",
        "            self.values[state] = max(state_values)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_env = gym.make(ENV_NAME)\n",
        "    agent = Agent()\n",
        "    writer = SummaryWriter(comment=\"-v-iteration\")\n",
        "\n",
        "    iter_no = 0\n",
        "    best_reward = 0.0\n",
        "    while True:\n",
        "        iter_no += 1\n",
        "        # 執行一次 value iteration\n",
        "        agent.play_n_random_steps(100)\n",
        "        agent.value_iteration()\n",
        "\n",
        "        # 驗證目前表現（玩 TEST_EPISODES 次的平均 reward）\n",
        "        reward = 0.0\n",
        "        for _ in range(TEST_EPISODES):\n",
        "            reward += agent.play_episode(test_env)\n",
        "        reward /= TEST_EPISODES\n",
        "        writer.add_scalar(\"reward\", reward, iter_no)\n",
        "        if reward > best_reward:\n",
        "            print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
        "            best_reward = reward\n",
        "        if reward > 0.80:\n",
        "            print(\"Solved in %d iterations!\" % iter_no)\n",
        "            break\n",
        "    writer.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyJ9kRKhrAjp",
        "outputId": "8a04cc7a-ff3d-4d56-89a5-3340ee260f50"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best reward updated 0.000 -> 0.150\n",
            "Best reward updated 0.150 -> 0.300\n",
            "Best reward updated 0.300 -> 0.350\n",
            "Best reward updated 0.350 -> 0.400\n",
            "Best reward updated 0.400 -> 0.450\n",
            "Best reward updated 0.450 -> 0.650\n",
            "Best reward updated 0.650 -> 0.700\n",
            "Best reward updated 0.700 -> 0.750\n",
            "Best reward updated 0.750 -> 0.850\n",
            "Solved in 113 iterations!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q iteration code"
      ],
      "metadata": {
        "id": "UNuqKE8V5JTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# copied and modified from the rlbook Chap5: frozenlake_q_iteration.py\n",
        "import gymnasium as gym\n",
        "import collections\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "ENV_NAME = \"FrozenLake-v1\"\n",
        "GAMMA = 0.9\n",
        "TEST_EPISODES = 20\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.env = gym.make(ENV_NAME)\n",
        "        self.state, _ = self.env.reset()\n",
        "        self.rewards = collections.defaultdict(float)\n",
        "        self.transits = collections.defaultdict(collections.Counter)\n",
        "        self.values = collections.defaultdict(float) # 注意：這裡的 value 是 action value\n",
        "\n",
        "    def play_n_random_steps(self, count):\n",
        "        for _ in range(count):\n",
        "            action = self.env.action_space.sample()\n",
        "            new_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "            is_done = terminated or truncated\n",
        "            self.rewards[(self.state, action, new_state)] = reward\n",
        "            self.transits[(self.state, action)][new_state] += 1\n",
        "            if is_done:\n",
        "                self.state,_ = self.env.reset()\n",
        "            else:\n",
        "                self.state = new_state\n",
        "\n",
        "    def select_action(self, state):\n",
        "        best_action, best_value = None, None\n",
        "        for action in range(self.env.action_space.n):\n",
        "            action_value = self.values[(state, action)]\n",
        "            if best_value is None or best_value < action_value:\n",
        "                best_value = action_value\n",
        "                best_action = action\n",
        "        return best_action\n",
        "\n",
        "    def play_episode(self, env):\n",
        "        total_reward = 0.0\n",
        "        state, _ = env.reset()\n",
        "        while True:\n",
        "            action = self.select_action(state)\n",
        "            new_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            is_done = terminated or truncated\n",
        "            self.rewards[(state, action, new_state)] = reward\n",
        "            self.transits[(state, action)][new_state] += 1\n",
        "            total_reward += reward\n",
        "            if is_done:\n",
        "                break\n",
        "            state = new_state\n",
        "        return total_reward\n",
        "\n",
        "    def value_iteration(self):\n",
        "        # 改 iterate over Q(s,a)\n",
        "        for state in range(self.env.observation_space.n): # over s\n",
        "            for action in range(self.env.action_space.n): # over a\n",
        "                action_value = 0.0\n",
        "                target_counts = self.transits[(state, action)]\n",
        "                total = sum(target_counts.values())\n",
        "                for tgt_state, count in target_counts.items():\n",
        "                    reward = self.rewards[(state, action, tgt_state)]\n",
        "                    best_action = self.select_action(tgt_state)\n",
        "                    action_value += (count / total) * (reward + GAMMA * self.values[(tgt_state, best_action)])\n",
        "                self.values[(state, action)] = action_value # dict: (s,a) -> q\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_env = gym.make(ENV_NAME)\n",
        "    agent = Agent()\n",
        "    writer = SummaryWriter(comment=\"-q-iteration\")\n",
        "\n",
        "    iter_no = 0\n",
        "    best_reward = 0.0\n",
        "    while True:\n",
        "        iter_no += 1\n",
        "        agent.play_n_random_steps(100)\n",
        "        agent.value_iteration()\n",
        "\n",
        "        reward = 0.0\n",
        "        for _ in range(TEST_EPISODES):\n",
        "            reward += agent.play_episode(test_env)\n",
        "        reward /= TEST_EPISODES\n",
        "        writer.add_scalar(\"reward\", reward, iter_no)\n",
        "        if reward > best_reward:\n",
        "            print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
        "            best_reward = reward\n",
        "        if reward > 0.80:\n",
        "            print(\"Solved in %d iterations!\" % iter_no)\n",
        "            break\n",
        "    writer.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "865ZmkrcvInW",
        "outputId": "e4074086-dcde-47f4-9c98-a74df0a9a3e6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best reward updated 0.000 -> 0.300\n",
            "Best reward updated 0.300 -> 0.550\n",
            "Best reward updated 0.550 -> 0.650\n",
            "Best reward updated 0.650 -> 0.700\n",
            "Best reward updated 0.700 -> 0.800\n",
            "Best reward updated 0.800 -> 0.850\n",
            "Solved in 36 iterations!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WlvYRL7O2_SW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}